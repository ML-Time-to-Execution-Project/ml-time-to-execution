{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "hidden_size1 = 50\n",
    "hidden_size2 = 20\n",
    "censor_weight = 0.1\n",
    "time_weight = 1\n",
    "num_epochs = 10\n",
    "input_size = 102"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_x = StandardScaler()\n",
    "sc_time = StandardScaler()\n",
    "class Standardized:\n",
    "    def __call__(self, sample):\n",
    "        inputs, time_label, censor_label = sample\n",
    "        sample = sc_x.transform(inputs), sc_time.transform(time_label), censor_label\n",
    "        return sample\n",
    "\n",
    "class ToTensor:\n",
    "    def __call__(self,sample):\n",
    "        inputs, time_label, censor_label = sample\n",
    "        return torch.from_numpy(inputs), torch.from_numpy(time_label), torch.from_numpy(censor_label)\n",
    "        # return torch.Tensor(inputs), torch.Tensor(time_label), torch.Tensor(censor_label)\n",
    "\n",
    "\n",
    "class OrderBookDataset(Dataset):\n",
    "    def __init__(self, transform=None, path1=f'D:\\Postgraduate_Course_Information/Director Peng/ML_Time_to_Execution/execution_torch_code/test_data/buy_x_train.csv',\n",
    "                           path2=f'D:/Postgraduate_Course_Information/Director Peng/ML_Time_to_Execution/execution_torch_code/test_data/buy_y_train.csv'):\n",
    "        # data loading\n",
    "        x = np.loadtxt(path1, dtype=np.float32, delimiter=',')\n",
    "        y = np.loadtxt(path2, dtype=np.float32, delimiter=',')\n",
    "        \n",
    "        # self.x = torch.from_numpy(x)\n",
    "        # self.y_time = torch.unsqueeze(torch.from_numpy(y[:, 0]),dim=1)\n",
    "        # self.y_censor = torch.unsqueeze(torch.from_numpy(y[:, 1]),dim=1)\n",
    "        self.x = x\n",
    "        self.y_time = np.log(y[:, 0]).reshape(-1,1)\n",
    "        self.y_censor = y[:, 1].reshape(-1,1)      \n",
    "        self.n_samples = y.shape[0]\n",
    "        self.transform = transform\n",
    "    def __getitem__(self, index, ):\n",
    "        sample = self.x[index].reshape(-1,102), self.y_time[index].reshape(-1,1), self.y_censor[index].reshape(-1,1)\n",
    "        if self.transform:\n",
    "            sc_x.fit(self.x)\n",
    "            sc_time.fit(self.y_time)\n",
    "            sample = self.transform(sample)\n",
    "        return sample\n",
    "        # dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "composed = torchvision.transforms.Compose([Standardized(),ToTensor()])\n",
    "# composed = torchvision.transforms.Compose([ToTensor()])\n",
    "dataset = OrderBookDataset(transform=composed)\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=50, shuffle=True, num_workers=0)\n",
    "\n",
    "total_samples = len(dataset)\n",
    "n_iterations = math.ceil(total_samples / 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 102])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs, time_label, censor_label = dataset[2]\n",
    "inputs.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model building\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, num_size=2):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.l1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.l2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.l3 = nn.Linear(hidden_size2, num_size)\n",
    "        self.norm1 = nn.BatchNorm1d(input_size)\n",
    "        self.norm2 = nn.BatchNorm1d(hidden_size1)\n",
    "        self.norm3 = nn.BatchNorm1d(hidden_size2)\n",
    "        \n",
    "        nn.init.xavier_uniform_(self.l1.weight)\n",
    "        nn.init.xavier_uniform_(self.l2.weight)\n",
    "        nn.init.xavier_uniform_(self.l3.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.norm1(x)\n",
    "        out = self.l1(out)\n",
    "        out = self.norm2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.l2(out)\n",
    "        out = self.norm3(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.l3(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function\n",
    "model = NeuralNet(input_size, hidden_size1, hidden_size2)\n",
    "criterion1 = nn.MSELoss()\n",
    "criterion2 = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=(0.9, 0.999), eps=1e-08, weight_decay=0)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = list(enumerate(dataloader))\n",
    "a[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\APP\\Anaconda3\\envs\\Financial\\lib\\site-packages\\torch\\nn\\modules\\loss.py:520: UserWarning: Using a target size (torch.Size([30, 1])) that is different to the input size (torch.Size([30])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Target size (torch.Size([30, 1])) must be the same as input size (torch.Size([30]))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15880/768299009.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0mloss1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtime_label\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[0mloss2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcensor_label\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;31m# print(loss1.size())\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\APP\\Anaconda3\\envs\\Financial\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\APP\\Anaconda3\\envs\\Financial\\lib\\site-packages\\torch\\nn\\modules\\loss.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    702\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    703\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 704\u001b[1;33m         return F.binary_cross_entropy_with_logits(input, target,\n\u001b[0m\u001b[0;32m    705\u001b[0m                                                   \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    706\u001b[0m                                                   \u001b[0mpos_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\APP\\Anaconda3\\envs\\Financial\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mbinary_cross_entropy_with_logits\u001b[1;34m(input, target, weight, size_average, reduce, reduction, pos_weight)\u001b[0m\n\u001b[0;32m   2978\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2979\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2980\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Target size ({}) must be the same as input size ({})\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2981\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2982\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary_cross_entropy_with_logits\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduction_enum\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Target size (torch.Size([30, 1])) must be the same as input size (torch.Size([30]))"
     ]
    }
   ],
   "source": [
    "# loop iteration\n",
    "for epochs in range(num_epochs):\n",
    "    for i, (inputs, time_label, censor_label) in enumerate(dataloader):\n",
    "        # forward backward update\n",
    "        inputs = torch.squeeze(inputs)\n",
    "        # print(inputs.size())\n",
    "        time_label = torch.squeeze(time_label,1)\n",
    "        # print(time_label.size())\n",
    "        censor_label = torch.squeeze(censor_label,1)\n",
    "        # forward\n",
    "        y_predicted = model(inputs)\n",
    "        # print(y_predicted.size())\n",
    "        out1 = y_predicted[:, 0] \n",
    "        out2 = y_predicted[:, 1]\n",
    "\n",
    "        # print(censor_label.size())\n",
    "        # print(torch.unsqueeze(torch.sigmoid(out1), dim=1).size())\n",
    "        # print(torch.unsqueeze(torch.sigmoid(out1), dim=1))\n",
    "        # print(torch.unsqueeze(censor_label, dim=1))\n",
    "\n",
    "\n",
    "        loss1 = criterion1(out1, time_label)\n",
    "        loss2 = criterion2(out2, censor_label)\n",
    "\n",
    "        # print(loss1.size())\n",
    "        # print(loss2.size())\n",
    "\n",
    "        #loss = criterion1(y_predicted, time_label)\n",
    "        loss = (time_weight * loss1 + censor_weight * loss2)\n",
    "        # print(loss.size())\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # update\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if (i + 1) % 30 == 0:\n",
    "            print(f'epoch:{epochs + 1}, loss = {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1, loss = 1.0909\n",
      "epoch:1, loss = 1.0757\n",
      "epoch:1, loss = 1.1086\n",
      "epoch:1, loss = 1.1195\n",
      "epoch:1, loss = 1.2189\n",
      "epoch:1, loss = 1.0248\n",
      "epoch:1, loss = 1.0543\n",
      "epoch:1, loss = 0.8942\n",
      "epoch:1, loss = 0.9429\n",
      "epoch:2, loss = 0.7653\n",
      "epoch:2, loss = 0.8082\n",
      "epoch:2, loss = 0.7275\n",
      "epoch:2, loss = 0.8951\n",
      "epoch:2, loss = 1.3327\n",
      "epoch:2, loss = 0.8477\n",
      "epoch:2, loss = 0.8035\n",
      "epoch:2, loss = 0.8568\n",
      "epoch:2, loss = 0.8837\n",
      "epoch:3, loss = 1.1029\n",
      "epoch:3, loss = 1.1755\n",
      "epoch:3, loss = 0.7664\n",
      "epoch:3, loss = 0.9559\n",
      "epoch:3, loss = 0.9884\n",
      "epoch:3, loss = 1.0379\n",
      "epoch:3, loss = 0.6356\n",
      "epoch:3, loss = 0.9902\n",
      "epoch:3, loss = 0.8851\n",
      "epoch:4, loss = 0.9355\n",
      "epoch:4, loss = 0.6988\n",
      "epoch:4, loss = 1.2170\n",
      "epoch:4, loss = 1.0475\n",
      "epoch:4, loss = 0.8060\n",
      "epoch:4, loss = 0.8609\n",
      "epoch:4, loss = 1.0854\n",
      "epoch:4, loss = 0.7107\n",
      "epoch:4, loss = 0.8318\n",
      "epoch:5, loss = 0.6062\n",
      "epoch:5, loss = 1.0860\n",
      "epoch:5, loss = 0.9039\n",
      "epoch:5, loss = 1.1394\n",
      "epoch:5, loss = 0.9231\n",
      "epoch:5, loss = 0.6422\n",
      "epoch:5, loss = 1.0528\n",
      "epoch:5, loss = 0.9778\n",
      "epoch:5, loss = 0.7572\n",
      "epoch:6, loss = 0.7941\n",
      "epoch:6, loss = 0.9307\n",
      "epoch:6, loss = 1.3192\n",
      "epoch:6, loss = 0.8174\n",
      "epoch:6, loss = 0.6234\n",
      "epoch:6, loss = 1.0236\n",
      "epoch:6, loss = 0.7647\n",
      "epoch:6, loss = 0.9826\n",
      "epoch:6, loss = 1.0589\n",
      "epoch:7, loss = 0.6351\n",
      "epoch:7, loss = 0.6291\n",
      "epoch:7, loss = 0.9460\n",
      "epoch:7, loss = 0.9513\n",
      "epoch:7, loss = 0.6963\n",
      "epoch:7, loss = 1.1038\n",
      "epoch:7, loss = 1.0650\n",
      "epoch:7, loss = 0.8736\n",
      "epoch:7, loss = 0.6147\n",
      "epoch:8, loss = 0.8796\n",
      "epoch:8, loss = 0.9671\n",
      "epoch:8, loss = 0.7310\n",
      "epoch:8, loss = 0.7239\n",
      "epoch:8, loss = 0.7610\n",
      "epoch:8, loss = 1.2336\n",
      "epoch:8, loss = 0.8188\n",
      "epoch:8, loss = 0.8034\n",
      "epoch:8, loss = 1.1953\n",
      "epoch:9, loss = 0.9245\n",
      "epoch:9, loss = 0.7810\n",
      "epoch:9, loss = 0.9565\n",
      "epoch:9, loss = 0.6152\n",
      "epoch:9, loss = 0.9996\n",
      "epoch:9, loss = 0.9531\n",
      "epoch:9, loss = 0.9444\n",
      "epoch:9, loss = 1.0272\n",
      "epoch:9, loss = 0.6643\n",
      "epoch:10, loss = 0.8217\n",
      "epoch:10, loss = 0.8774\n",
      "epoch:10, loss = 0.8508\n",
      "epoch:10, loss = 0.8792\n",
      "epoch:10, loss = 0.8136\n",
      "epoch:10, loss = 0.8464\n",
      "epoch:10, loss = 1.0511\n",
      "epoch:10, loss = 0.8705\n",
      "epoch:10, loss = 0.7481\n"
     ]
    }
   ],
   "source": [
    "# loop iteration\n",
    "for epochs in range(num_epochs):\n",
    "    for i, (inputs, time_label, censor_label) in enumerate(dataloader):\n",
    "        # forward backward update\n",
    "        inputs = torch.squeeze(inputs)\n",
    "        # print(inputs.size())\n",
    "        time_label = torch.squeeze(time_label,1)\n",
    "        # print(time_label.size())\n",
    "        censor_label = torch.squeeze(censor_label,1)\n",
    "        # forward\n",
    "        y_predicted = model(inputs)\n",
    "        # print(y_predicted.size())\n",
    "        out1 = torch.unsqueeze(y_predicted[:, 0],1)\n",
    "        # print(out1.size())\n",
    "        out2 = torch.unsqueeze(y_predicted[:, 1],1)\n",
    "\n",
    "        # print(censor_label.size())\n",
    "        # print(torch.unsqueeze(torch.sigmoid(out1), dim=1).size())\n",
    "        # print(torch.unsqueeze(torch.sigmoid(out1), dim=1))\n",
    "        # print(torch.unsqueeze(censor_label, dim=1))\n",
    "\n",
    "\n",
    "        loss1 = criterion1(out1, time_label)\n",
    "        loss2 = criterion2(out2, censor_label)\n",
    "\n",
    "        # print(loss1.size())\n",
    "        # print(loss2.size())\n",
    "\n",
    "        # loss = criterion1(y_predicted, time_label)\n",
    "        loss = (time_weight * loss1 + censor_weight * loss2)\n",
    "        # print(loss.size())\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # update\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if (i + 1) % 30 == 0:\n",
    "            print(f'epoch:{epochs + 1}, loss = {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "test_data/buy_y_test.csv not found.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2000/1055432713.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# test result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloadtxt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'test_data/buy_y_test.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m','\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mx_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloadtxt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'test_data/buy_x_test.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m','\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mx_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m102\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0my_time_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\APP\\Anaconda3\\envs\\Financial\\lib\\site-packages\\numpy\\lib\\npyio.py\u001b[0m in \u001b[0;36mloadtxt\u001b[1;34m(fname, dtype, comments, delimiter, converters, skiprows, usecols, unpack, ndmin, encoding, max_rows, like)\u001b[0m\n\u001b[0;32m   1065\u001b[0m             \u001b[0mfname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos_fspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1066\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_is_string_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1067\u001b[1;33m             \u001b[0mfh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_datasource\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1068\u001b[0m             \u001b[0mfencoding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'encoding'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'latin1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1069\u001b[0m             \u001b[0mfh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\APP\\Anaconda3\\envs\\Financial\\lib\\site-packages\\numpy\\lib\\_datasource.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(path, mode, destpath, encoding, newline)\u001b[0m\n\u001b[0;32m    191\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m     \u001b[0mds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataSource\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdestpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 193\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnewline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\APP\\Anaconda3\\envs\\Financial\\lib\\site-packages\\numpy\\lib\\_datasource.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(self, path, mode, encoding, newline)\u001b[0m\n\u001b[0;32m    531\u001b[0m                                       encoding=encoding, newline=newline)\n\u001b[0;32m    532\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 533\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%s not found.\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    534\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    535\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: test_data/buy_y_test.csv not found."
     ]
    }
   ],
   "source": [
    "# test result\n",
    "y_test = np.loadtxt(f'test_data/buy_y_test.csv', dtype=np.float32, delimiter=',')\n",
    "x_test = np.loadtxt(f'test_data/buy_x_test.csv', dtype=np.float32, delimiter=',')\n",
    "x_test = torch.from_numpy(sc_x.transform(x_test.reshape(-1,102)))\n",
    "y_time_test = torch.from_numpy(sc_time.transform(np.log(y_test[:, 0].reshape(-1,1))))\n",
    "y_censor_test = torch.from_numpy(y_test[:, 1].reshape(-1,1))\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_predicted = model(x_test)\n",
    "    y_time_predicted = y_predicted[:,0].reshape(-1,1)\n",
    "    y_censor_predicted = y_predicted[:,1]\n",
    "    y_predicted_cls = torch.sigmoid(y_censor_predicted).round()\n",
    "    censor_acc = (y_predicted_cls == y_censor_test).sum()/float(y_censor_test.shape[0])\n",
    "    # print(y_predicted_cls.eq(y_censor_test).sum())\n",
    "    print(y_time_predicted)\n",
    "    print(y_time_test)\n",
    "    print(y_predicted_cls.size())\n",
    "    print(y_censor_test.shape[0])\n",
    "    time_loss = criterion2(y_time_predicted, y_time_test)\n",
    "    print(y_predicted_cls)\n",
    "    print(f'censor accuracy = {censor_acc:4f}, time loss = {time_loss:4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5424, 0.6318, 0.5679,  ..., 0.5142, 0.5483, 0.5526])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_censor_predicted\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "326f373c6e0d43889a92efc6bf1e375f946ae88ffdce40ee1caa691dc5b3ab7b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('Financial': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
